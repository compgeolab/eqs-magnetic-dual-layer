% Define some custom math commands
\newcommand{\norm}[1]{\lVert #1 \rVert}
\newcommand{\vecbf}[1]{\vec{\mathbf{#1}}}
\newcommand{\uvec}[1]{\hat{\mathbf{#1}}}
\newcommand{\bvec}[1]{\bar{\mathbf{#1}}}
\newcommand{\mat}[1]{\bar{\bar{\mathbf{#1}}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

Potential field data often need to be interpolated onto a regular grid at constant height before further application, such as modelling crustal structures and geological interpretation. However, many gridding methods do not consider the variable survey heights which are typical of airborne data. Additionally, most methods do not take advantage of the fact that potential fields are harmonic functions. For example, the total-field magnetic anomaly is harmonic when the magnitude of the anomalous field is much smaller than the magnitude of the geomagnetic field, which is true for most ground and airborne surveys in which the total-field anomaly is observed \citep{Blakley1995}.

A widely used approach that addresses these issues is the equivalent sources (also known as equivalent layer) technique, first introduced by \citet{Dampney1969} and based on potential theory \citep{Kellogg1967}. This method approximates any harmonic function as the sum of discrete source effects, which are then used to predict the potential field in unobserved locations. However, estimating the source coefficients that best fit the observed data is computationally demanding and inherently non-unique. Since its introduction, numerous adaptations of the equivalent source technique have been developed to improve the computational efficiency and accuracy, such as: \citet{Leao1989}, \citet{Cordell1992}, \citet{Mendona1994}, \citet{Guspi2009}, \citet{Li2010}, \citet{OliveiraJr2013}, \citet{Siqueira2017}, \citet{Jirigalatu2019}, \citet{Mendona2020}, \citet{Li2020}, \citet{Soler2021}, \citet{Takahashi2022} and \citet{Piauilino2024}. A comprehensive review of the equivalent sources technique was undertaken by \citet{OliveiraJr2023} and we refer readers to it for more information.

For magnetic data in particular, the recent study of \citet{Li2020} identified that incorporating an additional deeper layer of equivalent sources improved the model fit to the data, particularly for the long-wavelength components. \citet{Li2020} fit both shallow and deep equivalent source layers to the observed data simultaneously using prisms as the sources. This required a depth-weighting factor to prevent the shallow layer coefficients from dominating, due to the sensitivity matrix elements of the shallow layer being much larger than those associated with the deep layer. Fitting both layers at once also significantly increases the computational cost of the inversion.

A strong motivation for accepting such a cost is the ability to calculate the amplitude of the anomalous magnetic field from the total-field anomaly observations. The anomalous field is the magnetic field produced by crustal sources and the total-field anomaly is approximately the projection of this field onto the direction of the geomagnetic field \citep{Blakley1995}. Although 3D non-linear inversions of total-field anomaly are known to be sensitive to the often unknown magnetisation direction, \citet{Li2010_remanent} and later \citet{HidalgoGato2021} show that inversions of the amplitude of the anomalous magnetic field are much less sensitive to uncertainty in the magnetisation direction. This reduced sensitivity occurs because the amplitude of the magnetic anomaly vector depends only weakly on the source magnetisation direction \citep{Nabighian1984, Haney2003}. Additionally, \citet{Melo2021} demonstrates the amplitude of the anomalous field can be useful for interpreting magnetic data at low latitudes, where other techniques, such as reduction-to-the-pole, tend to be unstable.

A caveat of using the equivalent sources techniques is the need for manual selection of hyperparameters, such as the damping regularisation parameter and the depth of the equivalent sources. Cross-validation is a statistical technique, often used in machine learning, for assessing how well a model fits data that was not used to train it. \citet{Geisser1975} introduced the K-fold method to reduce computational load compared to other cross-validation methods. In equivalent sources processing, \citet{Soler2021} applied K-fold cross-validation to estimate the damping parameter and the depth of the equivalent sources when gridding gravity data over Australia. However, \citet{Roberts2017} showed that when observations are spatially auto-correlated, meaning that nearby observations have similar values, K-fold cross-validation tends to underestimate the interpolation error and leads to overfitting of the model. This is a common feature of both potential field data due to their smooth nature and also airborne data in general for their spatial bias along flight lines. To combat this, \citet{Roberts2017} developed the blocked K-fold method, specifically designed for cross-validation of spatially auto-correlated data.

This study presents the dual-layer gradient-boosted magnetic equivalent sources method to fit total-field anomaly data and predict the amplitude of the anomalous magnetic field. This creates a dataset less dependent on the direction of Earth’s main field and crustal magnetisation. The approach mitigates the computational challenges of applying a dual-layer approach to millions of data points by orders of magnitude and reduces border effects. This enhances both the efficiency and the accuracy by:

\begin{enumerate}
  \item Fitting a deep equivalent source layer of magnetic dipoles to a reduced set of observed data by block-averaging the data before the inversion.
  \item Fitting a second, shallower layer of equivalent sources to the residuals from the deep equivalent sources using the gradient-boosting method of \citet{Soler2021}.
  \item Composing the final prediction of either total-field anomaly or amplitude of the anomalous field by the summation of the predictions from both layers on a regular grid.
  \item Using block K-fold cross-validation to assist in the determination of the optimal damping parameter and depth of the equivalent sources.
\end{enumerate}

\noindent
This approach demonstrates the effectiveness of the dual-layer method on synthetic data and real aeromagnetic data from Antarctica.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methodology}

The total-field anomaly $\Delta T(x, y, z)$, observed at a point $(x, y, z)$ on a Cartesian right-handed coordinate system with $x$ pointing eastward and $z$ pointing upward, is the difference between the measured norm of the total magnetic field $\vecbf{T}(x, y, z)$ and the norm of the regional reference field $\vecbf{F}(x, y, z)$, usually represented by the International Geomagnetic Reference Field (IGRF), at the time of measurement:

\begin{equation}
    \Delta T(x, y, z) =
    \norm{\vecbf{T}(x, y, z)}
    - \norm{\vecbf{F}(x, y, z)}
    \ .
\end{equation}

\noindent
The total magnetic field $\vecbf{T}$ is the sum of the anomalous magnetic field vector $\vecbf{B}$, which is produced by lithospheric sources, and the regional field vector $\vecbf{F}$ \citep{Blakley1995, Langel1998, OliveiraJr2015}. The total-field anomaly $\Delta T$ can therefore be written as

\begin{align}
    \label{eq:tfa-definition}
    \Delta T(x, y, z) &=
    \norm{\vecbf{F}(x, y, z) + \vecbf{B}(x, y, z)} - \norm{\vecbf{F}(x, y, z)}\ ,
    \\
    &= \left(
        \vecbf{F} \cdot \vecbf{F} 
        + 2\vecbf{F} \cdot \vecbf{B} 
        + \vecbf{B} \cdot \vecbf{B}
        \right)^{\frac{1}{2}} 
    - \norm{\vecbf{F}}
    \ .
\end{align}

\noindent
For the majority of crustal anomalies measured by airborne and shipborne surveys, $\norm{\vecbf{B}} \ll \norm{\vecbf{F}}$ thus $\norm{\vecbf{B}}$ is negligible in the equation above. Consequently, $\Delta T$ can be approximated as \citep{Blakley1995}

\begin{align}
    \Delta T &\approx \left(
        \vecbf{F} \cdot \vecbf{F} 
        + 2\vecbf{F} \cdot \vecbf{B}
    \right)^{\frac{1}{2}} 
    - \norm{\vecbf{F}} \ ,
    \\
    &= \norm{\vecbf{F}} \left(
        1 + 2 \dfrac{
            \vecbf{F} \cdot \vecbf{B}
        }{
            \vecbf{F} \cdot \vecbf{F}
        }
    \right)^{\frac{1}{2}} 
    - \norm{\vecbf{F}} \ .
    \label{eq:}
\end{align}

\noindent
Using the Taylor series expansion $\left( \sqrt{1 + x} = 1 + \frac{x}{2} - \frac{x^2}{8} + \cdots \right)$ up to the first-order term, the square-root can be approximated:

\begin{align}
    \Delta T & \approx \norm{\vecbf{F}} \left(
        1 + \dfrac{
            \vecbf{F} \cdot \vecbf{B}
        }{
            \vecbf{F} \cdot \vecbf{F}
        }
    \right) 
    - \norm{\vecbf{F}} \ ,
    \\
    &=
    \dfrac{\vecbf{F} \cdot \vecbf{B}}{\norm{\vecbf{F}}} \ ,
    \\
\label{eq:tfa_dot_product}
    \Delta T & \approx \vecbf{B} \cdot \uvec{F}
    \ ,
\end{align}

\noindent
in which $\uvec{F}$ is a unit vector in the same direction as the regional field. Additionally, $\vecbf{F}$ can be considered constant on the scale of a single survey. Thus, $\Delta T$ is approximately a harmonic function in most crustal magnetic studies \citep{Blakley1995,OliveiraJr2015}.


\subsection{Equivalent Source Technique}

Consider a set of $N$ observations of total-field anomaly $\Delta T$ at locations $(x_i, y_i, z_i)$ that are organized into a vector $\bvec{d}^o$ of observed data. The equivalent source technique assumes these observations can be approximated by a harmonic function \textcolor{orange}{$d_i = d(x_i, y_i, z_i)$} which is the sum of $M$ discrete source effects \citep{Dampney1969, Cordell1992}:

\begin{equation}
\label{eq:eqs_technique}
\textcolor{orange}{d_i} = 
\textcolor{orange}{d (x_i, y_i, z_i)} = \sum_{j=1}^{M} a_{j}\left(x_i, y_i, z_i\right) \textcolor{teal}{c_j}
\ ,
\end{equation}

\noindent
in which \textcolor{orange}{$d_i$} is the predicted data calculated at the Cartesian coordinates $(x_i, y_i, z_i)$. 
The function $a_j(x_i, y_i, z_i)$ is the effect of the $j$-th source with unitary physical property, located at the Cartesian coordinates $(x'_j, y'_j, z'_j)$, calculated at the observation point $(x_i, y_i, z_i)$. The coefficients \textcolor{teal}{$c_j$} are the norm of the magnetic moment of the $j$-th source \textcolor{teal}{$\norm{\vecbf{m}_j} = m_j$}.

For magnetic surveys, \textcolor{orange}{$d (x_i, y_i, z_i)$} from Equation~\ref{eq:eqs_technique} is equal to \textcolor{orange}{$\Delta T(x_i, y_i, z_i) = \vecbf{B}(x_i, y_i, z_i) \cdot \uvec{F}$} (Equation~\ref{eq:tfa_dot_product}). Therefore, function $a_j(x_i, y_i, z_i) = \vecbf{b}_j(x_i, y_i, z_i) \cdot \uvec{F}$, in which $\vecbf{b}_j(x_i, y_i, z_i)$ is the magnetic field of the $j$-th source with unit magnetic moment $\hat{\mathbf{m}}_j$.
When the equivalent sources are dipoles, the magnetic field for a unit magnetic moment is given by \citep{Blakley1995}:

\begin{equation}
    \vecbf{b}_{ij} = \vecbf{b}_j (x_i, y_i, z_i) = C_m \dfrac{
        3 \left(\uvec{m}_j \cdot \uvec{l}_{ij}\right) 
        \uvec{l}_{ij} - \uvec{m}_j
    }{{l_{ij}}^3}
    \ ,
    \label{eq:magnetic_field}
\end{equation}

\noindent
in which $C_m = \frac{\mu_0}{4 \pi} = 10^{-7} \ \text{Hm}^{-1}$ is the proportionality constant, $\mu_0$ is the magnetic permeability of free space, $\uvec{m}_j$ is a unit vector of dipole moment, $\uvec{l}_{ij}$ is the unit vector between the $i$-th observation point and the $j$-th equivalent source and $l_{ij}$ is the distance between the $i$-th observation point and the $j$-th source:

\begin{equation}
    l_{ij}= \sqrt{(x_i - x_j')^2 + (y_i - y_j')^2 + (z_i - z_j')^2}
    \ .
\end{equation}

As a result, Equation~\ref{eq:eqs_technique} becomes

\begin{equation}
\label{eq:tfa_eqs}
\textcolor{orange}{d_i} =
\sum_{j=1}^{M} \left(\ 
    \vecbf{b}_{ij} \cdot \hat{\mathbf{F}}
\ \right)\ 
\textcolor{teal}{m_j}
\ .
\end{equation}

\noindent
For $N$ observation points, Equation~\ref{eq:tfa_eqs} can be arranged in a linear system which can be expressed in matrix form:

\begin{equation}
\textcolor{orange}{\begin{bmatrix}
    d_1 \\ d_2 \\ \vdots \\ d_N
\end{bmatrix}_{Nx1}} = \begin{bmatrix}
    \vecbf{b}_{11} \cdot \uvec{F} & \vecbf{b}_{12} \cdot \uvec{F} & \cdots & \vecbf{b}_{1M} \cdot \uvec{F} \\
    \vecbf{b}_{21} \cdot \uvec{F} & \vecbf{b}_{22} \cdot \uvec{F} & \cdots & \vecbf{b}_{2M} \cdot \uvec{F} \\
    \vdots & \vdots & \vdots & \vdots \\
    \vecbf{b}_{N1} \cdot \uvec{F} & \vecbf{b}_{N2} \cdot \uvec{F} & \cdots & \vecbf{b}_{NM} \cdot \uvec{F} \\
\end{bmatrix}_{NxM} \textcolor{teal}{\begin{bmatrix}
    m_1 \\ m_2 \\ \vdots \\ m_j
\end{bmatrix}_{Mx1}} \ ,
\end{equation}

\begin{equation}
    \textcolor{orange}{\bvec{d}} = \mat{A} \textcolor{teal}{\bvec{c}}
    \ ,
    \label{eq:forward-matrices}
\end{equation}

\noindent
in which \textcolor{orange}{$\bvec{d}$} is the $N \times 1$ predicted data vector, \textcolor{teal}{$\bar{\mathbf{c}}$} is the $M \times 1$ parameter vector and $\mat{A}$ is the $N \times M$ sensitivity (Jacobian) matrix.

The inverse problem of the equivalent sources method consists of finding the parameter vector \textcolor{teal}{$\bar{\mathbf{c}}$} that best fits the observed data \textcolor{orange}{$\bvec{d}^o$} subject to minimum norm constraints. 
This can be achieved by minimizing the damped least-squares goal function

\begin{equation}
\label{eq:goal_function}
    \phi(\textcolor{teal}{\bar{\mathbf{c}}}) = \textcolor{orange}{\bar{\mathbf{r}}^T\bar{\mathbf{r}}} + \mu\ \textcolor{teal}{\bar{\mathbf{c}}^T\bar{\mathbf{c}}}
    \ ,
\end{equation}

\noindent
in which $\mu$ is the positive regularisation parameter and  \textcolor{orange}{$\bar{\mathbf{r}}$} is the residual vector:

\begin{equation}
    \label{eqs:resdiual}
    \textcolor{orange}{\bvec{r}} = \textcolor{orange}{\bvec{d}^o} - \textcolor{orange}{\bvec{d}}
    \ .
\end{equation}

\noindent
The goal function from Equation~\ref{eq:goal_function} can  be expanded to give

\begin{equation}
    \phi (\textcolor{teal}{\bvec{c}}) = 
    \left(\textcolor{orange}{\bvec{d}^o} - \mat{A} \textcolor{teal}{\bvec{c}}\right)^T 
    \left(\textcolor{orange}{\bvec{d}^o} - \mat{A} \textcolor{teal}{\bvec{c}}\right) 
    + \mu\ \textcolor{teal}{\bvec{c}^T\bvec{c}}
    \ .
\end{equation}

\noindent
The minimum of the goal function can be found by taking its gradient and equating it to the null vector:

\begin{equation}
    \nabla \phi = 
    2 \mat{A}^T \mat{A} \textcolor{teal}{\bvec{c}} 
    - 2\mat{A}^T \textcolor{orange}{\bvec{d}^o} 
    + 2 \mu \textcolor{teal}{\bvec{c}}
    = \bvec{0}
    \ .
\end{equation}

\noindent
This equation can be rearranged to express the normal equation system

\begin{equation}
    \left(\ \mat{A}^T \mat{A} + \mu \mat{I}\ \right) 
    \textcolor{teal}{\bvec{c}} =
    \mat{A}^T\textcolor{orange}{\bvec{d}^o}
    \ ,
    \label{eq:normal_equations}
\end{equation}

\noindent
which can be solved for \textcolor{teal}{$\bvec{c}$}. Once \textcolor{teal}{$\bvec{c}$} has been estimated, Equation~\ref{eq:eqs_technique} can be used to forward model the total-field anomaly at any $(x, y, z)$ location.

It is also possible to use the estimated parameters \textcolor{teal}{$\bvec{c}$} to calculate transformations of the potential field, such as upward continuation, directional derivatives and reduction-to-the-pole, by adjusting function $a_j(x_i, y_i, z_i)$ appropriately.
However, in order to calculate the amplitude of the anomalous magnetic field vector $\vecbf{B}$ (Equation~\ref{eq:tfa-definition}), the forward modelling equation must be modified to

\begin{equation}
\textcolor{orange}{\norm{\vecbf{B}(x,y,z)}} = \left\lVert 
    \sum_{j=1}^{M} \vecbf{b}_{j}(x,y,z)\ \textcolor{teal}{m_j} 
\right\rVert
\ .
\end{equation}


\subsection{Dual-Layer Concept}

The use of two layers, one shallow and one deep, was introduced by \citet{Li2020} to predict the three components of the anomalous field ($\vecbf{B}$) from total-field anomaly observations. \citet{Li2020} found that having an additional deeper layer of equivalent sources improved the model fit to the data, particularly for long‐wavelength fields. \citet{Li2020} fit both layers to the observed data simultaneously. Due to the sensitivity matrix elements associated with the shallow layer being much larger than the elements associated with the deep layer, their method required the use of a depth-weighting factor to keep the shallow layer coefficients from dominating.

Here the dual-layer method is modified to separate the deep and shallow sources into two different sets of parameters, \textcolor{teal}{$\bar{\mathbf{c}}^d$} and \textcolor{teal}{$\bar{\mathbf{c}}^s$}, respectively.
The forward modelling Equation~\ref{eq:forward-matrices} is then modified to

\begin{equation}
    \textcolor{orange}{\bvec{d}} = \mat{A}^d \textcolor{teal}{\bvec{c}^d}
    +  \mat{A}^s \textcolor{teal}{\bvec{c}^s}
    \ ,
\end{equation}

\noindent
in which $\mat{A}^d$ and $\mat{A}^s$ are the sensitivity matrices for the deep and shallow sources, respectively.
Instead of solving for both $\textcolor{teal}{\bvec{c}^d}$ and $\textcolor{teal}{\bvec{c}^s}$ simultaneously, we first estimate the deep source coefficients $\textcolor{teal}{\bvec{c}^d}$ and then estimate the shallow source coefficients $\textcolor{teal}{\bvec{c}^s}$ sequentially.

\begin{algorithm}[!htb]
  \setstretch{1.5}
  Establish the geographic bounding box (region) of the data
  \;
  Add an amount of padding to the edges of the bounding box
  \;
  Divide the padded region into blocks of equal size
  \;
  For each block, calculate the median $(x, y, z)$ coordinates and the median data value of the observations that fall within the respective block
  \;
  \BlankLine
  \setstretch{1}
  \caption{The block-averaging method.}
  \label{alg:block_averaging}
\end{algorithm}

To estimate the deep layer coefficients first, the short-wavelength information has to be removed from the observed data to allow the deep layer to only capture the long-wavelength components, rather than both short- and long-wavelength components. This is achieved by block-averaging the observed line data, as described in Algorithm~\ref{alg:block_averaging}, to arrive at the $N^d \times 1$ block-averaged observed data vector $\textcolor{orange}{\bvec{d}^{od}}$. The deep equivalent sources are then placed one beneath each block-averaged data point at a given relative depth following \citet{Soler2021}, providing $M^d$ deep sources with $M^d = N^d$. The block-averaged observed data $\textcolor{orange}{\bvec{d}^{od}}$ are then used to estimate the $M^d$ deep layer coefficients by solving the normal equation system

\begin{equation}
    \left(\ {\mat{A}^d}^T \mat{A}^d + \mu \mat{I}\ \right) 
    \textcolor{teal}{\bvec{c}^d} =
    {\mat{A}^d}^T\textcolor{orange}{\bvec{d}^{od}}
    \ .
\end{equation}

\noindent
Thus, another advantage of using the block-averaged data for fitting the deep layer is the reduced computational load because $N^d \ll N$, as well as the increased stability of the model. 

The estimated dipole moments of the deep layer \textcolor{teal}{$\bvec{c}^d$} are then used to calculate a predicted total-field anomaly \textcolor{orange}{$\bvec{d}^d$} using Equation~\ref{eq:tfa_eqs} on all of the $N$ original observation points. Subsequently, a deep layer residual vector is calculated given by

\begin{equation}
    \textcolor{orange}{\bar{\mathbf{r}}^d} = 
    \textcolor{orange}{\bar{\mathbf{d}}^o} - \mat{A}^d \textcolor{teal}{\bvec{c}^d}
    \ .
    \label{eq:deep_residual}
\end{equation}

\noindent
The deep sources residuals $\textcolor{orange}{\bvec{r}^d}$ are then used as the observations to estimate the shallow sources coefficients $\textcolor{teal}{\bvec{c}^s}$ in a separate inverse problem. 

The positions of the shallow sources are calculated by block-averaging the data coordinates using a block size equal to the desired grid spacing \citep{Soler2021}, leading to $M^s < N$ sources. It is worth emphasising that only the source coordinates undergo block-averaging and not the observed data themselves. Fitting multiple data points and sources for a single grid point did not significantly improve the model fit in practice and would unnecessarily increase the computational cost. The equivalent sources were placed directly beneath each blocked-averaged coordinate. This layout was also preferred over a regular grid of equivalent sources, as sources positioned without directly overlying data worsened the model fit \citep{Soler2021}. 

The $M^s$ shallow layer coefficients are estimated by fitting the $N$ deep layer residuals (\textcolor{orange}{$\bar{\mathbf{r}}^d$}). Since $N$ can be large for real-world airborne surveys (on the order of tens to hundreds of millions of observations), the fitting employs the gradient-boosted equivalent source technique from \citet{Soler2021}, which is explained in Section~\ref{sec:gradient-boosting}. 

Once the coefficients for both the deep and shallow layers are estimated, the total-field anomaly can be predicted by combining the predictions of both layers:

\begin{equation}
    \label{eq:tfa_eqs_dual_layer}
    \textcolor{orange}{\Delta T (x, y, z)} = 
    \sum_{j=1}^{M^d} \left(
        \vecbf{b}^d_j(x, y, z) \cdot \uvec{F}
    \right) \textcolor{teal}{m_j^d}
    + \sum_{j=1}^{M^s} \left(
        \vecbf{b}^s_j(x, y, z) \cdot \uvec{F}
    \right) \textcolor{teal}{m_j^s}
  \ ,
\end{equation}

\noindent
in which $\vecbf{b}_j^d$ and $\vecbf{b}_j^s$ are the magnetic field vectors with unit magnetic moment for the $j$-th deep and shallow sources, respectively. Likewise, the amplitude of the anomalous field can also be predicted by combing the predictions of both layers:

\begin{equation}
  \textcolor{orange}{\norm{\vecbf{B}(x,y,z)}} =
  \left\lVert \sum_{j=1}^{M^d} \vecbf{b}^d_{j}(x,y,z)\ \textcolor{teal}{m^d_j}
  +
  \sum_{j=1}^{M^s}  \vecbf{b}^s_{j}(x,y,z)\ \textcolor{teal}{m^s_j}
  \right\rVert
  \ .
\end{equation}

\noindent
A summary of this dual-layer method proposed here is given in Algorithm~\ref{alg:dual_layer}.

\begin{algorithm}[!htb]
  \setstretch{1.5}
  Block-average the observed data
  \;
  Place $M^d$ deep equivalent sources one beneath each block-averaged data point at a given relative depth
  \;
  Estimate $M^d$ deep layer coefficients \textcolor{teal}{$\bar{\mathbf{c}}^d$} using the block-averaged data
  \;
  Use the estimated dipole moments of the deep layer \textcolor{teal}{$\bar{\mathbf{c}}^d$} to predict the total-field anomaly \textcolor{orange}{$\bar{\mathbf{d}}^d$} using Equation~\ref{eq:tfa_eqs} on all of the $N$ original observation points
  \;
  Calculate a deep layer residual vector using Equation~\ref{eq:deep_residual}
  \;
  Block-average the data coordinates by a block size equal to the desired grid spacing
  \;
  Place the shallow equivalent sources beneath the newly block-averaged data coordinates
  \;
  Estimate the shallow layer coefficients \textcolor{teal}{$\bar{\mathbf{c}}^s$} by fitting the $N$ deep layer residuals \textcolor{orange}{$\bar{\mathbf{r}}^d$}
  \;
   Predict the total-field anomaly by combining the predictions of both layers using Equation~\ref{eq:tfa_eqs_dual_layer}.
  \BlankLine
  \setstretch{1}
  \caption{The dual-layer equivalent source method.}
  \label{alg:dual_layer}
\end{algorithm}


\subsection{Gradient-Boosted Equivalent Sources}
\label{sec:gradient-boosting}

Estimating \textcolor{teal}{$\bar{\mathbf{c}}$} using the damped least-squares solution (Equation~\ref{eq:normal_equations}) is computationally demanding, especially on a regional scale, due to the many data points. To overcome this problem, \citet{Soler2021} adapted the gradient-boosting method from \citet{Friedman2001}, which allows additive models to be fit iteratively. Using this method, the shallow source coefficients (\textcolor{teal}{$\bar{\mathbf{c}}^s$}) are estimated in overlapping windows and carried out iteratively. At each iteration, the coefficients are estimated by fitting the residuals from the previous iteration within a given window. The coefficients are then used to predict the field across the entire dataset, after which the residuals are updated before proceeding to the next window. The window size should be selected to maximise usage of the available RAM on the user's machine. Following \citet{Friedman2002}, \citet{Soler2021} also iterated through the windows randomly to improve the model fit to the data. The gradient-boosted equivalent sources method reduces the computational load by solving numerous smaller damped least-squares problems rather than one large problem. This method is applied to the shallow layer of equivalent sources because it is fitted to the entire dataset, which can contain millions of observations in real airborne surveys. An outline of the method is presented in Algorithm~\ref{alg:gradient_boosting}.

\begin{algorithm}[!h]
    \setstretch{1.5}
    Determine a set of $Q$ windows overlapping by 50\% that are equal in size and cover the whole survey area
    \;
    Shuffle the order of the windows in the set of windows
    \;
    Initialise the residuals vector with the observed data \textcolor{orange}{$\mathbf{r^1}$} = \textcolor{orange}{$\mathbf{d^o}$}
    \;
    \For{ $q = 1$ \KwTo $Q$}{
        Fit the sources inside window $q$ to the subset of residuals \textcolor{orange}{$\mathbf{r^q}$} that fall within the window to obtain the coefficient vector \textcolor{teal}{$\mathbf{c^q}$}
        \;
        Use Equation~\ref{eq:tfa_eqs} calculate a vector of predicted data \textcolor{orange}{$\mathbf{d^q}$} on all of the $N$ observation points
        \;
        Update the residuals to \textcolor{orange}{$\mathbf{r^{q+1}}$} = \textcolor{orange}{$\mathbf{r^q}$} - \textcolor{orange}{$\mathbf{d^q}$}
        \;
    }
    Predict new data values using \textcolor{orange}{$\mathbf{d}$} = $\sum\limits_{q=1}^{Q} A^q$ \textcolor{teal}{$\mathbf{c^q}$}
    \;
    \BlankLine
    \setstretch{1}
    \caption{The gradient-boosted equivalent sources method.}
    \label{alg:gradient_boosting}
\end{algorithm}

\subsection{Cross-Validation and Model Selection}

The equivalent sources model requires careful selection of appropriate values for the damping parameter, $\mu$ (see Equation~\ref{eq:goal_function}), and the relative depth of the equivalent sources. These two parameters, referred to as hyperparameters of the inversion, significantly influence the smoothing of the model predictions. It is therefore crucial to select values for these hyperparameters that yield stable predictions in the unobserved locations when using the equivalent sources for interpolation. Selecting the optimal values of damping and relative depth requires the establishment of a metric of how well a model with a given combination of these hyperparameters can be interpolated.

Cross-validation is a statistical technique commonly used in machine learning to obtain a metric of how successful a model is at fitting data that was not used to train the model. Data are split into two subsets: one for model training and one for model testing. This prevents overfitting by ensuring the training set is independent to the testing set. \citet{Geisser1975} introduced K-fold cross-validation to reduce the computational load compared to other cross-validation methods. In K-fold cross-validation, data are split into K equally-sized folds. The folds 2 to $K$ are used as the training set to construct the model and the remaining fold (fold 1) is used as the testing set to validate the model \citep{Jung2017}. This is then repeated by using a subsequent fold for testing and the remaining folds for training until each fold has been used as a testing set.

\citet{Roberts2017} introduced the blocked versions of cross-validation methods for when data are spatially auto-correlated. This is necessary for when observations taken at close points tend to have similar values, which is often the case for both potential field data due to their smooth nature and airborne data in particular for their spatial bias along flight lines. Block K-Fold cross validation also tends to favour smoother models which helps reduce the effects of high frequency noise in final products. In the block K-fold cross-validation method, data (black dots in Figure~\ref{fig:BK-CV}) are first divided into non-overlapping spatial blocks of a specified size (orange blocks in Figure~\ref{fig:BK-CV}). These blocks are then randomly assigned to $K$ folds, ensuring each fold contains approximately the same number of data points. Data from folds 2 to $K$ are assigned to the training set (blue dots in step 1 of Figure~\ref{fig:BK-CV}), whilst the remaining fold is assigned to the testing set (red dots in step 1 of Figure~\ref{fig:BK-CV}). The training set is fitted with the equivalent source model using Equation\ref{eq:normal_equations} or the gradient-boosted equivalent sources method in order to estimate the parameters \textcolor{teal}{$\bar{\mathbf{c}}$}. The model is then used to predict \textcolor{orange}{$\mathbf{d_{test}}$}, the total-field anomaly (\textcolor{orange}{$\Delta T (x, y, z)$}), on the coordinates from the testing set using Equation~\ref{eq:tfa_eqs} or the equivalent for the gradient-boosted equivalent sources.

\begin{figure}[tb]
  \centering
  \includegraphics[width=1\linewidth]{figures/bk_cv.png}
  \caption{
    The block K-fold cross-validation method. The black dots are the data, the orange blocks are the non-overlapping spatial blocks of a specified size, the blue dots are the training sets for each iteration and the red dots are the testing sets for each fold.
    }
  \label{fig:BK-CV}
\end{figure}

The model fit to the data can be assessed though the Root Mean Square Error (RMSE) calculated between the observed data from the testing set (\textcolor{orange}{$\mathbf{d^o_{test}}$}) and the predicted total-field anomaly also on the testing set coordinates (\textcolor{orange}{$\mathbf{d_{test}}$}):

\begin{equation}
    \label{eq:rmse}
    \text{RMSE}_k = \sqrt{\dfrac{\sum\limits_{i=1}^L \left(\textcolor{orange}{{d^o_{test_i}}} - \textcolor{orange}{{d_{test_i}}}\right)^2}{L}}
    \ ,
\end{equation}

\noindent
for $L$ number of testing points. This block K-fold cross-validation process is iterated until all the folds have been used for both testing and training (see Figure~\ref{fig:BK-CV}). The overall cross-validated RMSE is determined by taking the average of all the $\text{RMSE}_k$ values across the folds. This block K-fold cross-validation is summarised in Algorithm~\ref{alg:BK-CV}.

\begin{algorithm}[!h]
    \setstretch{1.5}
    Split the data into blocks of a given size
    \;
    Split the blocks randomly into K folds, with roughly the same number of data per fold
    \;
    \For{each fold $k$}{
        Assign fold $k$ to the testing set and the remaining folds to the training set
        \;
        Estimate the parameters \textcolor{teal}{$\bar{\mathbf{c}}_k$} using the data from the training set and Equation~\ref{eq:normal_equations} or the gradient-boosted equivalent sources method
        \;
        Predict the total-field anomaly \textcolor{orange}{$\Delta T (x, y, z)$} on the coordinates of the testing set using Equation~\ref{eq:tfa_eqs}
        \;
        Calculate the $\text{RMSE}_k$ between the observed data from the testing set (\textcolor{orange}{$\mathbf{d^{o}_{test}}$}) and the predicted total-field anomaly (\textcolor{orange}{$\mathbf{d_{test}}$}) using Equation~\ref{eq:rmse}
        \;
    }
    Calculate the cross-validated RMSE by taking the average of all the $\text{RMSE}_k$ calculated for each fold.
    \BlankLine
    \setstretch{1}
    \caption{The block K-fold cross-validation method.}
    \label{alg:BK-CV}
\end{algorithm}

To determine the optimal hyperparameters for each layer, a range of values for damping and relative depth is systematically generated. \citet{Dampney1969} suggests bounds of 2.5 to 6 times the average distance to the nearest neighbouring data points for the depth of equivalent sources. This range is utilised as a starting point for the depth of the deep equivalent sources to ensure the regional long-wavelength signals are captured. However, it is important to note that this metric was only used as a starting point to define an initial range for testing and was expanded when needed based on the cross-validation results. For the shallow layer, a range of relative depths between the data heights and the deep layer are employed. Once these ranges are defined, a comprehensive set of all the possible combinations of these hyperparameters is created. For each combination, the block K-fold cross-validation method (Algorithm~\ref{alg:BK-CV}) is employed to determine the corresponding cross-validated RMSE, which serves as the performance metric. The combination that yields the smallest cross-validated RMSE is then selected as the optimal set of parameters for the final equivalent source model.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Synthetic Data Application}

To assess the accuracy of the interpolations, the method was applied to synthetic datasets containing magnetic sources with varying shapes, sizes, depths and induced magnetisations. To ensure consistency with real-world conditions, the synthetic dataset was simulated using the flight lines from the ICEGRAV 2013 aeromagnetic dataset \citep{ICEGRAV_data}, as detailed in Section~\ref{sec:real_application}. These flight line coordinates were projected using the Universal Polar Stereographic (UPS) projection, specifically for the South Pole region. The Earth's main magnetic field direction was set to match the IGRF orientation at the time of the original measurements, with an inclination of -65$^\circ$ and declination of 35$^\circ$, consistent with the real aeromagnetic dataset. A regional magnetic field was modelled using the ``checkerboard'' function of Verde  \citep{verde} on a regularly spaced grid covering the entire survey area, located at 60 km beneath the surface with an oscillating height of 15 km. The magnetisation for the regional field was specified to have an inclination of -50$^\circ$, declination of 40$^\circ$ and magnitude of $5 \times 10^{12}$Am$^2$. A zero-mean pseudo-random Gaussian noise, with a standard deviation of 5 nT, was also applied to the regional field. Additionally, a deep dipole was added to study the effects of a truncated long-wavelength signal (see Section~\ref{sec:truncated_regional} for further details). This dipole was placed at coordinates (2250 km, 2730 km, -70 km), with a magnetisation of 55$^\circ$ inclination, 45$^\circ$ declination and $2 \times 10^{13}$Am$^2$ magnitude. The descriptions of the shallower synthetic sources can be found in Table~\ref{table:shallow_sources}, providing their shape, location and magnetisation.

\begin{table}[tb!]
\centering
\begin{tabular}{c c c c c c}
\toprule
 & Centre Location & \multicolumn{3}{c}{Magnetisation} \\
\cmidrule(lr){2-2} \cmidrule(lr){3-5}
Source & [x, y, depth] (km) & Inclination ($^\circ$) & Declination ($^\circ$) & Magnitude (Am$^2$) \\
\midrule
Oval & [1930, 3030, 6] & 75 & 60 & $2 \times 10^{10}$ \\
Irregular & [1960, 2980, 0.8] & -60 & 45 & $3 \times 10^{9}$ \\
Small Dipole & [1950, 2785, 0.5] & 45 & -65 & $5 \times 10^{10}$ \\
Small Dipole	& [2100, 3011, 0.5] & 45 & -65 & $5 \times 10^{10}$ \\
Small Dipole & [2110, 2955, 0.5] & 45 & -65 & $5 \times 10^{10}$ \\
Small Dipole & [2150, 2750, 0.5] & 45 & -65 & $5 \times 10^{10}$ \\
Linear (Dyke) & [2200, 2950, 1] & -70 & 80 & $1 \times 10^{9}$ \\
Linear (Dyke) & [2220, 2870, 5] & -70 & 80 & $2 \times 10^{9}$ \\
Irregular & [2230, 2820, 0.9] & 50 & 70 & $5 \times 10^{9}$ \\
Regional Dipole & [2260, 2830, 8] & -70 & 40 & $1 \times 10^{10}$ \\
\bottomrule
\end{tabular}
\caption{Descriptions of the shallower synthetic sources.}
\label{table:shallow_sources}
\end{table}

\subsection{Single- and Dual-Layer Comparison}
\label{sec:single_vs_dual}

\begin{figure}[tb!]
\centering
\includegraphics[width=1\linewidth]{paper/figures/cv_synthetic.png}
\caption{
    Cross-validation results for selecting the optimal hyperparameters in the synthetic data test. The combination with the lowest RMSE is marked with a white star. a) The single-layer model with the lowest RMSE obtained at a relative depth of 35 km and damping of $1 \times 10^2$. b) The deep layer of the dual-layer model with the lowest RMSE obtained at a relative depth of 147 km and damping of $1 \times 10^1$. c) The shallow layer of the dual-layer model with the lowest RMSE at a relative depth of 17 km and damping of $1 \times 10^2$.
}
\label{fig:cv_synthetic}
\end{figure}

\begin{figure}[tb!]
\centering
\includegraphics[width=1\linewidth]{figures/single_layer_synthetic.png}
\caption{
    Synthetic data and the predictions using a single-layer of equivalent sources. a) observed total-field anomaly of the synthetic data on the survey lines from the ICEGRAV survey \citep{ICEGRAV_data}, b) total-field anomaly prediction on the survey lines, c) residual between the observed (a) and predicted (b) total-field anomaly on the survey lines with a RMSE of 8.5 nT and a histogram of the residuals in the bottom left; d) true total-field anomaly on a regular grid with 5 km spacing, e) predicted total-field anomaly on regular grid with 5 km spacing, f) residual between the true (d) and predicted (e) total-field anomaly on a regular grid with 5 km spacing and an RMSE of 13.2 nT and a histogram of the residuals in the bottom left; g) true amplitude of the anomalous magnetic field on a regular grid with 5 km spacing, h) predicted amplitude of the anomalous magnetic field on a regular grid with 5 km spacing, i) residual between the true (g) and predicted (h) amplitude of the anomalous magnetic field on a regular grid with 5 km spacing and an RMSE of 14.1 nT and a histogram of the residuals in the bottom left.
}
\label{fig:single_layer_synthetic}
\end{figure}

\begin{figure}[tb!]
\centering
\includegraphics[width=1\linewidth]{figures/dual_layer_synthetic.png}
\caption{
    Synthetic data and the predictions using the dual-layer equivalent sources. a) observed total-field anomaly of the synthetic data on the survey lines from the ICEGRAV survey \citep{ICEGRAV_data}, b) total-field anomaly prediction on the survey lines, c) residual between the observed (a) and predicted (b) total-field anomaly on the survey lines with a RMSE of 4.7 nT and a histogram of the residuals in the bottom left; d) true total-field anomaly on a regular grid with 5 km spacing, e) predicted total-field anomaly on regular grid with 5 km spacing, f) residual between the true (d) and predicted (e) total-field anomaly on a regular grid with 5 km spacing and an RMSE of 8.2 nT, as well as a histogram of the residuals in the bottom left; g) true amplitude of the anomalous magnetic field on a regular grid with 5 km spacing, h) predicted amplitude of the anomalous magnetic field on a regular grid with 5 km spacing, i) residual between the true (g) and predicted (h) amplitude of the anomalous magnetic field on a regular grid with 5 km spacing and an RMSE of 8.2 nT, as well as a histogram of the residuals in the bottom left.
}
\label{fig:dual_layer_synthetic}
\end{figure}

To assess the performance of a single- versus dual-layer of equivalent sources, the single-layer of equivalent sources was first applied using the gradient-boosted equivalent sources method (see Algorithm~\ref{alg:gradient_boosting}). To ensure optimal selection of the hyperparameters, the block K-fold cross-validation method (see Algorithm~\ref{alg:BK-CV}) was employed, using a block size of 30 km $\times$ 30 km over a wide range of parameter values. The optimal hyperparameters, which resulted in the lowest cross-validated RMSE, included a relative depth of 35 km (see Figure~\ref{fig:cv_synthetic}a). These optimised parameters were then used in the gradient-boosted equivalent sources with a window size of 250 km $\times$ 250 km. To further minimise the residuals of the inversion, the iteration over the windows was performed twice. In all test cases, the inclination of the equivalent sources was \ang{90} and the declination was \ang{0}. To obtain the regular grid predictions, the model was first fit to the observed line data and then used to predict on a regular grid with 5 km spacing.

The resulting single-layer predictions for the total-field anomaly along the survey lines, total-field anomaly on a regular grid, and the amplitude of the anomalous magnetic field on a regular grid are presented in Figure~\ref{fig:single_layer_synthetic}. The prediction of the total-field anomaly along the survey lines (Figure~\ref{fig:single_layer_synthetic}b) provides an overall good fit to the data, with an RMSE of 8.5 nT. However, it underestimates the magnitude of the four small, shallow dipoles, a pattern consistent with the predictions for both the total-field anomaly (Figure~\ref{fig:single_layer_synthetic}e) and the amplitude of the anomalous magnetic field (Figure~\ref{fig:single_layer_synthetic}h). Furthermore, the single-layer model introduces undulating ripple effects at the edges of the sources, especially along the dykes. It also generates edges effects at the borders of the survey area (Figure~\ref{fig:single_layer_synthetic}e). These results highlight a key challenge when fitting both the short- and long-wavelength components simultaneously with a single layer.

To explore the potential benefits of a dual-layer model, the dual-layer equivalent source method (see Algorithm~\ref{alg:dual_layer}) was applied to the same synthetic dataset. The observed data were block-averaged using different block spacings in order to determine the ideal size that is able to isolate the long-wavelength signals for the deep layer to fit. When the block spacing is too small, some of the short-wavelength signals are captured, resulting in the deep layer not capturing all of the regional long-wavelength signals due to trying to fit both long- and short-wavelengths. When the block spacing is too large, not all of the regional signals are captured either. The chosen optimal block spacing for this synthetic data was 25 km $\times$ 25 km. To allow for more data points that fall along the survey boundary to be used, a padding of $0.2 \times $ the block spacing was added to the data border. The use of padding reduces the potential for edge effects at the borders that were seen in the single-layer model. Padding was employed rather than extending the sources beyond the survey bounds because the latter would often worsen the edge effects, whereas padding allows for more blocked data points along the survey edges. Using the block K-fold cross-validation method (Algorithm~\ref{alg:BK-CV}) with a block size of 100 km $\times$ 100 km, the optimal hyperparameters for the deep layer, which resulted in the lowest cross-validated RMSE, included a relative depth of approximately 147 km, $7.5 \times$ the grid spacing (see Figure~\ref{fig:cv_synthetic}b). A larger block size was used for the deep layer because of the longer wavelength of the observed signals. The shallow layer was then fitted to the residuals from the deep layer fit in order to focus on fitting the shorter-wavelength signals, as well as correct potential errors caused by the deep layer. The gradient-boosted equivalent sources method (Algorithm~\ref{alg:gradient_boosting}) was utilised with a window size of 250 km $\times$ 250 km, matching the single-layer model. To optimise the shallow layer’s hyperparameters, the block K-fold cross-validation method was applied with a block size of 30 km $\times$ 30 km (the same as the single-layer model) to account for local variations in the data. The optimal hyperparameters for the shallow layer, resulting in the lowest cross-validated RMSE, included a relative depth of 17 km (see Figure~\ref{fig:cv_synthetic}c). The iteration over the windows of the gradient boosting method was repeated a second time to minimise potential errors in the initial windows selected in the iteration, similar to the single-layer model.

The dual-layer results for the total-field anomaly along the survey lines, total-field anomaly on a regular grid, and the amplitude of the anomalous magnetic field on a regular grid are shown in Figure~\ref{fig:dual_layer_synthetic}. The dual-layer model provides a significantly improved prediction of the total-field anomaly along the survey lines (Figure~\ref{fig:dual_layer_synthetic} b), with an RMSE of 4.7 nT, particularly in capturing the four small dipoles. Additionally, the regional field prediction is much smoother, with reduced edge effects at the survey borders or along the dykes, for both the total-field anomaly and amplitude of the anomalous magnetic field. This will allow different surveys to be blended more easily. The dual-layer model successfully reduces the undulating ripple effects near the edges of the sources. However, the dual-layer method does face challenges when predicting on the regular grid (Figure~\ref{fig:dual_layer_synthetic} e and h). Specifically, the method struggles with artifacts related to the flight lines, particularly along the dykes. The gaps between the survey lines appear more pronounced in these areas, leading to visible discontinuities. This issue could be addressed by adjusting the relative depth of the shallow layer, but in doing so may impact the model fit for the small anomalies such as the small dipoles. Therefore, after using the block K-fold cross-validation to narrow the range of optimal values, visual inspection of the predictions is necessary in order to select the final value.

Overall, the dual-layer approach improves the model fit to the data and the interpolations, for both total-field anomaly and amplitude of the anomalous magnetic field, by allowing the deep layer to capture the regional, long-wavelength signals, whilst the shallow layer focuses on the short-wavelength signals and corrects potential errors from the deep layer. This combination enhances the model fit to the data when compared to the single-layer approach, illustrating the advantages of using multiple layers of equivalent sources to model multi-scale sources. Applying padding to the deep layer reduces the edge effects along the survey borders, a common artefact when using a single-layer model. The cross-validation method provides a more automated and efficient way to select optimal hyperparameters. Additionally, the use of the gradient boosting method reduces the computational load and the processing time.

\subsection{Truncated Long-wavelength Anomaly}
\label{sec:truncated_regional}

\begin{figure}[tb!]
\centering
\includegraphics[width=1\linewidth]{figures/truncated_regional.png}
\caption{
    Synthetic data with a truncating regional dipole and the predictions using a single and dual-layer of equivalent sources. a) true total-field anomaly on a regular grid with 5 km spacing, b) predicted total-field anomaly on regular grid with 5 km spacing using a single-layer of equivalent sources, c) predicted total-field anomaly on a regular grid with 5 km spacing using the dual-layer of equivalent sources; d) true amplitude of the anomalous magnetic field on a regular grid with 5 km spacing, e) predicted amplitude of the anomalous magnetic field on a regular grid with 5 km spacing using a single-layer of equivalent sources, f) predicted amplitude of the anomalous magnetic field on a regular grid with 5 km spacing using the dual-layer of equivalent sources.
}
\label{fig:truncated_regional}
\end{figure}

\begin{figure}[tb!]
\centering
\includegraphics[width=1\linewidth]{figures/truncated_regional_rmses.png}
\caption{
    The Root Mean Square Error (RMSE) from the predictions of the total-field anomaly (solid lines) and amplitude of the anomalous magnetic field (dashed lines) on a regular grid (with 5 km spacing) as the regional dipole was shifted outside the survey boundary (a). The x-axis in (a) shows the y-coordinate at the top of the regional dipole. The RMSEs from the single-layer approach are shown in orange and the RMSEs from the dual-layer approach are shown in green. The shaded grey region in (a) highlights when the regional dipole was truncated and could partially be seen within the survey bounds. (b-d) shows the true total-field anomaly on regular grids (with 5 km spacing) with the regional dipole located at different y-coordinates as it was shifted outside the survey bounds. The y-coordinates at the top of the regional dipole are 2730 km (b), 2800 km (c) and 2880 km (d).
}
\label{fig:truncated_regional_rmses}
\end{figure}

The impact of a truncated long-wavelength signal was also investigated further after observing the single-layer approach struggled to accurately interpolate the regional field. To assess this effect, both the single- and dual-layer models were applied while progressively moving a dipolar source with a long wavelength signal out of the survey boundary. An example of the truncated regional dipole is illustrated in Figure~\ref{fig:truncated_regional}). Both single- and dual-layer approaches appear to underestimate the magnitude of the truncated regional signal. Furthermore, the single-layer approach creates edge effects along the survey boundary resulting from the truncated regional signal (see Figure~\ref{fig:truncated_regional}b). On the contrary, the dual-layer approach mitigated these edge effects (see Figure~\ref{fig:truncated_regional}c), which was also noted in Section~\ref{sec:single_vs_dual}.

As the signal of the regional dipole became increasingly truncated, the RMSE for both models increased and then decreased back to the initial RMSE as the dipole influence became less visible (Figure~\ref{fig:truncated_regional_rmses}). The RMSE for the amplitude of the anomalous magnetic field (the grey shaded region of Figure~\ref{fig:truncated_regional_rmses}) significantly increased in comparison to the total-field anomaly. The dual-layer approach consistently exhibited a lower RMSE compared with the single-layer approach, suggesting it provides more reliable model fit overall in the presence of a truncated regional field signal. The impact of a truncated long-wavelength signal further highlights the importance of using a multi-layer approach when applying the equivalent sources technique.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Real Data Application}
\label{sec:real_application}

\begin{figure}[tb!]
\centering
\includegraphics[width=1\linewidth]{paper/figures/cv_real.png}
\caption{
    Cross-validation results for selecting the optimal hyperparameters for the real data. The combination with the lowest RMSE is marked with a white star. a) The deep layer of the dual-layer model with the lowest RMSE obtained at a relative depth of 59 km and damping of $1 \times 10^1$. b) The shallow layer of the dual-layer model with the lowest RMSE at a relative depth of 11 km and damping of $1 \times 10^2$.
}
\label{fig:cv_real}
\end{figure}

\begin{figure}[tb!]
\centering
\includegraphics[width=1\linewidth]{figures/real_line_pred.png}
\caption{
    The real data and prediction on the flight lines using the dual-layer approach. The observed total-field anomaly data of the ICEGRAV survey \citep{ICEGRAV_data} (a), the prediction (b) on the same survey lines using the dual-layer of equivalent sources and the residual (c) between the observed and the predicted with an RMSE of 27.3 nT and a histogram of the residuals in the bottom left.
}
\label{fig:real_line_pred}
\end{figure}

The method was applied to the open-access aeromagnetic data from the ICEGRAV campaigns \citep{ICEGRAV_data}, which spanned from 2010 to 2013. The data covers parts of interior East Antarctica, including key areas such as the Dronning Maud Land ice stream systems and the Recovery Lakes drainage basin. This dataset was selected to showcase the effectiveness of the method on real-world data that features irregular survey flight lines, substantial spacing between those lines and varying line altitudes. The dataset’s distinctive characteristics, in terms of its vast geographical coverage, complex data collection and large number of data points (404,363 observations), provide a robust and challenging test case for evaluating the method's performance in complex conditions.

\begin{figure}[tb!]
\centering
\includegraphics[width=1\linewidth]{figures/real_grid_pred.png}
\caption{
    The real data predictions on a regular grid using the dual-layer approach. The prediction of the total-field anomaly (a) and amplitude of the anomalous magnetic field (b) of the ICEGRAV survey \citep{ICEGRAV_data} on a regular grid with 5km spacing. The survey lines are shown in white.
}
\label{fig:real_grid_pred}
\end{figure}

The coordinates of this dataset were first projected using the Universal Polar Stereographic (UPS) projection, specifically for the South Pole region. The dual-layer equivalent source method (Algorithm~\ref{alg:dual_layer}) could then be applied to the observed data, which have already undergone some preprocessing such as levelling and IGRF correction \citep{ICEGRAV_data}. To fit the deep equivalent source layer, the observed data was block-averaged using different block spacings to determine the ideal size for isolating most of the long-wavelength signals. A block spacing of 15 km $\times$ 15 km, with a padding of $ 0.3 \times $ the block spacing, was selected by visual inspection. The padding allows for additional data points along the survey boundary, reducing the potential for edge effects at the borders. 
The inclination of the equivalent sources was \ang{90} and the declination was \ang{0}.

The block K-fold cross-validation method (see Algorithm~\ref{alg:BK-CV}) was applied with a block size of 200 km $\times$ 200 km to determine the optimal hyperparameters which yield the lowest cross-validated RMSE. The chosen parameters included a relative depth of approximately 59 km (see Figure~\ref{fig:cv_real}a). The shallow layer was then fitted to the residuals from the deep layer using the gradient boosting method (see Algorithm~\ref{alg:gradient_boosting}) with a window size of 400 km $\times$ 400 km. The cross-validation method was utilised again to determine the optimal hyperparameters for the shallow layer. Using a block size of 20 km $\times$ 20 km, the chosen parameters with the lowest cross-validated RMSE included a relative depth of approximately 11 km (see Figure~\ref{fig:cv_real}b). The gradient-boosted equivalent sources method (see Algorithm~\ref{alg:gradient_boosting}) was applied twice to the shallow layer to minimise potential errors in the initially selected windows of the iterations. The results for the total-field anomaly along the survey lines are shown in Figure~\ref{fig:real_line_pred}. The dual-layer provides a good fit to the complicated dataset, with an RMSE of 27.3 nT.

The predictions of the total-field anomaly and amplitude of the anomalous magnetic field on a regular grid are shown in Figure~\ref{fig:real_grid_pred}. Using a machine with 128 Gigabytes of RAM and an Intel Core i9 9980XE 3GHz processor with 18 cores, the dual-layer approach took approximately 100 seconds to fit the 404,363 observed total-field anomaly data point and predict the amplitude of the anomalous field on a regular grid. The grid predictions reveal numerous points with very small magnitudes close to 0 nT, a pattern that is also present in the observed data. However, it remains uncertain whether these very small values represent real values or if they are artifacts introduced during the data preprocessing. If the latter is the case, more careful preprocessing may be necessary before applying the equivalent source method.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}

Different adaptations of the equivalent sources technique have been developed to improve the computational efficiency and fit to the data. However, many of these approaches still face challenges, such as the need for regularly gridded data at constant height, reliance on a stabilising parameter or the presence of border effects in the predictions. The dual-layer gradient-boosted equivalent sources tackles these limitations by:

\begin{enumerate}
    \item Using the dual-layer approach to improve the fit to the data and reduce the border effect.
    \item Using a two-step approach, block-averaging and the gradient-boosted equivalent sources method to reduce the computational load.
    \item Applying block K-fold cross-validation to guide optimal hyperparameter selection for the model.
\end{enumerate}

The synthetic data tests demonstrate the dual-layer approach enhances the model fit to the data and ability to interpolate over a single-layer approach by allowing the deeper layer to capture the regional, long-wavelength signals, whilst the shallower layer focuses on the short-wavelength signals. This approach also significantly improves predictions in cases where long-wavelength signals are truncated. Block-averaging the observed data reduces the number of sources needed in the deep layer, enabling it to focus on fitting the regional signals and decreasing the computational load. Applying padding to the deeper layer increases the number of data points along the survey border, reducing the potential for edge effects, a common issue with the equivalent sources technique. To further enhance the computation efficiency, the gradient-boosted equivalent sources technique was utilised for fitting the shallow layer. This approach solves several smaller damped least-squares problems rather than solving one large problem. Repeating the window iterations of the gradient-boosting algorithm twice helped to minimise the residuals.

Further exploration of the single-layer model demonstrates the inherent trade-off: when placing the equivalent source layer deeper, it captures the regional, long-wavelength signals more effectively but struggles with the short-wavelength signals. Conversely, placing the equivalent source layer shallower, improves the capture of the short-wavelength signals but compromises the ability to determine the regional, long-wavelength signals. This balancing act emphasises the limitations of using a single-layer approach in modelling multi-scale sources and underlines the need for using a multi-layer model.

An additional equivalent source layer that would act as an intermediate layer was tested on the synthetic datasets. However, the minimal improvement in model fit to the data did not justify the increase in computational load and decrease in stability for using three layers instead of two. Further exploration into using multiple equivalent source layers and the possible physical implications of the depth of these layers is ongoing.

Applying the dual-layer gradient-boosted equivalent sources to the ICEGRAV data demonstrated the method’s ability to successfully interpolate the data, as well as predict the amplitude of the anomalous magnetic field. However, any artefacts from the data preprocessing will still be present in the final predictions, emphasising the importance of careful preprocessing before applying any gridding method. While the real data application here involves a single survey, the method can be utilised for combining multiple datasets. However, care must be taken particularly when surveys have differing sampling rates. In such cases, we recommend applying the block-averaging method to each individual survey before merging, in order to prevent bias towards the higher sample frequency data. This consideration is important when extending the method to regional compilations or legacy datasets.

The use of block K-fold cross-validation has proven beneficial in narrowing the optimal range for the model hyperparameters. However, visual inspection of the predictions still remains necessary to refine the final selection. This is due to the interpolation sometimes imprinting survey line artifacts onto the regular grid prediction. Moreover, the current approach relies on a grid search which can be computationally expensive, particularly when testing a large range of hyperparameters. Future work could benefit from adaptive hyperparmeter optimisation methods, such as Bayesian optimisation, to improve the efficiency when exploring the parameter space. Further investigation into the ideal block size for cross-validation would also be beneficial, as well as the exploration of other spatial cross-validation methods that exist in the literature.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Data Availability Statement}

The Python source code used to produce all of the results and figures presented here are available at \url{https://github.com/\GitHubRepository} and
\url{https://doi.org/\ArchiveDOI} under the CC-BY license and the MIT open-source license. This study also made use of the following open-source scientific software: Numpy \citep{numpy} for linear algebra, matplotlib \citep{matplotlib} and PyGMT \citep{pygmt} for generating figures and maps, pyproj \citep{pyproj} for data projection, Pandas \citep{pandas} for manipulating tabular data, xarray \citep{xarray} for working with gridded data, Verde \citep{verde} for the moving windows and interpolation and Harmonica \citep{harmonica} for potential field data processing and modelling. The aeromagnetic data are available from the British Antarctic Survey \citep{ICEGRAV_data} under the UK Open Government Licence V3.0.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Acknowledgements}

We are indebted to the developers and maintainers of the open-source software without which this work would not have been possible. We are appreciative to editor Dr. Lindsey Heagy, reviewer Dr. Matthew Tankersley and an anonymous reviewer for their constructive comments. We would like to thank Dr. Tom Jordan for his advice and guidance on the different Antarctic aeromagnetic surveys from the British Antarctic Survey. We are also grateful to Prof. J\"{o}rg Ebbing, Dr. Santiago R. Soler and Gelson F. Souza-Junior for all of the insightful discussions which have helped shape this research. LU and IU were supported in part by start-up grant PRPI 22.1.09345.01.2 from Universidade de São Paulo. IU was also supported in part by grants from the Earth Science Research Group and the School of Environmental Science at the University of Liverpool.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{CRediT author Contributions}

\textbf{India Uppal:} Conceptualisation, Methodology, Software, Validation, Formal Analysis, Investigation, Resources, Data Curation, Writing - Original Draft, Writing - Review and Editing, Visualisation, Project administration.
\textbf{Leonardo Uieda:} Conceptualisation, Methodology, Software, Resources, Writing - Review and Editing.
\textbf{Vanderlei Coelho Oliverira Jr.:} Conceptualisation, Methodology, Writing - Review and Editing.
\textbf{Richard Holme:} Conceptualisation, Writing - Review and Editing.
